<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Soumya Rani</title>
  
  <meta name="author" content="Soumya Rani Samineni">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Soumya Rani Samineni</name>
              </p>
              <p>I am an ML Research Engineer at Quantiphi, Bangalore, where I work on Reinforcement learning for workforce optimisation.
                 Prior to this role, I held the position of Research Fellow at Microsoft Research, India with focus on desgning reinforcement Learning algorithms for Energy Grids.
                 I've also worked as an AI Engineer at AI Labs based out of Hyderabad, on development of Quadrupedal controller inspired by MIT Cheetah's Impedence control and also on Object Detection Models.
              </p>
              <p>
                I did my Masters in Computer Science and Engineering from <a href="https://www.csa.iisc.ac.in/"> Department of Computer Science and Automation, IISc Bangalore </a>, where I was advised by Prof <a href="https://shishirny.github.io/"> Shishir Kolathaya</a> and Prof <a href="https://www.csa.iisc.ac.in/~shalabh/">Shalabh Bhatnagar</a>.
                My Masters thesis titled Policy search using Dynamic Mirror Descent for offpolicy RL has received funding from <a href="https://cps.iisc.ac.in/"> Robert Bosch Center for Cyber Physical Systems (RBCCPS) </a>. As part of both <a href="https://www.csa.iisc.ac.in/~shalabh/"> </a>Stochastic Systems Lab and <a href="http://stochlab.github.io/"> Stochastic Robotics Lab</a> I have explored Reinforcement Learning for Robotics and Stochastic approximation. 
                <br> 
                <br> Before joining IISC, I have worked as an Assitant Executive Engineer(Civil) for Government of Telangana and I have done my bachelors in Civil Engineering from <a href="https://nitw.ac.in/"> National Institute of Technology Warangal</a>.
                <!---I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.--->
              </p>
              <p style="text-align:center">
                <a href="mailto:saminenisoumyarani@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Soumya-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/Research_Statement.pdf">Research_Statement</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=L0XiUr8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/soumya_samineni">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/soumyarani">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/soumya_oval.png"><img style="width:50%;max-width:100%" alt="profile photo" src="images/soumya_oval.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in reinforcement learning, machine learning, stochastic approximation, optimization, and deep learning. Much of my research is about novel RL algorithms that are optimal and sample efficient. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()"  >
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="https://soumyarani.github.io/DMD-MPC-RL/">
                1. <papertitle>Dynamic Mirror Descent based Model Predictive Control for Accelerating Robot Learning </papertitle>
              </a>
              <br>
              <strong>Soumya R Samineni*</strong>,Utkarsh Mishra*, P Goel, C Kunjeti, H Lodha, A Singh, A Sagi,             
              <a href= "https://www.csa.iisc.ac.in/~shalabh/">Shalabh Bhatnagar </a>, <a href="https://shishirny.github.io/"> Shishir Kolathaya </a> 
              <br> (*equal contribution) <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2022 &nbsp <font color="green"></font><br>
							<em>NIPS Deep RL Workshop</em>, 2021 &nbsp <font color="red"><strong>(Poster)</strong></font> <br>
							<em>NIPS Offline RL Workshop</em>, 2021 &nbsp <font color="red"><strong>(Poster)</strong></font><br>
              
              <br>
              <a href="https://soumyarani.github.io/DMD-MPC-RL/">project page</a>
              /
              <a href="https://arxiv.org/abs/2112.02999">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=Bj9dN1KNPAs">video</a>
							<!--/ -->
              <!-- <a href="https://github.com/soumyarani/mopac">code</a> -->
              <p></p>
              <p>Summary: Dynamic Mirror Descent is applied for an H step lookahead policy optimisation to augment the dataset for training an offpolicy RL, improving significantly the sample efficincy of Soft Actor Critic, widely used offpolicy RL algorithm. Further the proposed framework, DeMoRL generalises existing Model Based-Model Free (Mb-Mf) Approaches and acheives state of the art performance in Benchmark MuJoCo Tasks.</p>
            </td>
          </tr> 
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()" >
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="http://arxiv.org/abs/2110.12239">
                <papertitle>2. Policy Search using Dynamic Mirror Descent MPC for Model Free Off Policy RL </papertitle>
              </a>
              <br>
              <strong>Soumya R Samineni*</strong>,
              <em>Masters Thesis</em>, 2021 &nbsp <br>
              <br>
              <a href="">project page</a>
              /
              <a href="http://arxiv.org/abs/2110.12239">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=Bj9dN1KNPAs">video</a>
							/
              <a href="https://github.com/soumyarani/mopac">code</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>

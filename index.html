<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Soumya Rani</title>
  
  <meta name="author" content="Soumya Rani Samineni">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Soumya Rani Samineni</name>
              </p>
              <p>
                I am a second-year Ph.D. student at <strong>Arizona State University</strong> with 3+ years of professional experience and over 2 years of research experience in
                <strong>Artificial Intelligence (AI)</strong>, <strong>Reinforcement Learning (RL)</strong>, <strong>Large Language Models (LLMs)</strong>, <strong>Machine Learning (ML)</strong>,
                <strong>optimization</strong>, and <strong>robotics</strong>.
              </p>
            
              <p>
                I worked as an <strong>ML Research Engineer</strong> at <strong>Quantiphi, Bangalore</strong>, where I applied reinforcement learning to workforce optimization.
                I was a <strong>Research Fellow</strong> at <strong>Microsoft Research India</strong>, focusing on reinforcement learning algorithms for energy grids. I also contributed as an
                <strong>AI Engineer</strong> at <strong>AI Labs, Hyderabad</strong>, developing a quadrupedal controller inspired by MIT Cheetah’s impedance control and building object-detection models.
              </p>
              <p>
                I completed my Master’s in Computer Science and Engineering at the
                <a href="https://www.csa.iisc.ac.in/" target="_blank" rel="noopener">Department of Computer Science and Automation, IISc Bangalore</a>, advised by
                <a href="https://shishirny.github.io/" target="_blank" rel="noopener">Prof. Shishir Kolathaya</a> and
                <a href="https://www.csa.iisc.ac.in/~shalabh/" target="_blank" rel="noopener">Prof. Shalabh Bhatnagar</a>.
                My master’s thesis, <em>Policy Search using Dynamic Mirror Descent for Off-policy Reinforcement Learning</em>, was funded by the
                <a href="https://cps.iisc.ac.in/" target="_blank" rel="noopener">Robert Bosch Center for Cyber-Physical Systems (RBCCPS)</a>.
                During my time in the <a href="https://www.csa.iisc.ac.in/~shalabh/" target="_blank" rel="noopener">Stochastic Systems Lab</a> and the
                <a href="http://stochlab.github.io/" target="_blank" rel="noopener">Stochastic Robotics Lab</a>, I studied reinforcement learning for robotics and stochastic approximation.
                <br> 
                <br>Prior to IISc, I served as an Assistant Executive Engineer (Civil) for the Government of Telangana and hold a Bachelor's degree in Civil Engineering from
                <a href="https://nitw.ac.in/" target="_blank" rel="noopener">National Institute of Technology, Warangal (NITW)</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:saminenisoumyarani@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Soumya_CV.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=L0XiUr8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/soumya_samineni">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/soumyarani">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Soumya_oval.jpg"><img style="width:50%;max-width:100%" alt="profile photo" src="images/Soumya_oval.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:12px 20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I’m interested in Artificial Intelligence, with a focus on reinforcement learning, large language models, machine learning, optimization, and deep learning. My research spans reinforcement learning–based post-training methods for large language models, as well as studying their reasoning and planning limitations.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:12px 20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
          <tr>
            <td style="padding:12px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2505.13697">
                1. <papertitle>RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs</papertitle>
              </a>
              <br>
              <strong>Soumya Rani Samineni</strong>, Durgesh Kalwar, Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati
              <br>
              <em>NeurIPS 2025, LAW: Bridging Language, Agent, and World Models for Reasoning and Planning Workshop</em>, 2025
              <!--a href="https://arxiv.org/abs/2505.13697"></a-->
            </td>
          </tr>
          <tr>
            <td style="padding:12px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2510.18176">
                2. <papertitle>Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains</papertitle>
              </a>
              <br>
              <strong>Soumya Rani Samineni</strong>, Durgesh Kalwar, Vivek Gangal, Siddhant Bhambri, Subbarao Kambhampati
              <br>
              <em>NeurIPS 2025, 5th Workshop on Mathematical Reasoning and AI</em>, 2025
              <br>
              <!--a href="https://arxiv.org/pdf/2510.18176">arXiv</a-->
            </td>
          </tr>
          <tr>
            <td style="padding:12px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2504.09762">
                3. <papertitle>Stop Anthropomorphizing Intermediate Tokens as Reasoning/Thinking Traces!</papertitle>
              </a>
              <br>
              Subbarao Kambhampati, Kaya Stechly, Karthik Valmeekam, Levi Saldyt, Siddhant Bhambri, Vatsal Palod, Anuj Gundawar, <strong>Soumya Rani Samineni</strong>, Durgesh Kalwar, Uttaran Biswas
              <br>
              <em>NeurIPS 2025, Workshop on CogInterp: Interpreting Cognition in Deep Learning Models</em>, 2025
              <br>
              <!--a href="https://arxiv.org/pdf/2504.09762">arXiv</a-->
            </td>
          </tr>
          <tr>
            <td style="padding:12px 20px;width:100%;vertical-align:middle">
              <a href="https://patents.google.com/patent/US20240319718A1/en">
                4. <papertitle>System and Method for Intelligent Scheduling of Manufacturing Jobs</papertitle>
              </a>
              <br>
              Dagnachew Birru, Anirudh Deodhar, Achint Chaudhary, <strong>Soumya Rani Samineni</strong>
              <br>
              <em>US Patent Application US20240319718A1</em>, 2024
              <br>
              <!--a href="https://patents.google.com/patent/US20240319718A1/en">patent</a-->
            </td>
          </tr>
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()"  >
            <td style="padding:12px 20px;width:100%;vertical-align:middle">
              <a href="https://soumyarani.github.io/DMD-MPC-RL/">
                5. <papertitle>Dynamic Mirror Descent based Model Predictive Control for Accelerating Robot Learning </papertitle>
              </a>
              <br>
              <strong>Soumya R Samineni*</strong>,Utkarsh Mishra*, P Goel, C Kunjeti, H Lodha, A Singh, A Sagi,             
              <a href= "https://www.csa.iisc.ac.in/~shalabh/">Shalabh Bhatnagar </a>, <a href="https://shishirny.github.io/"> Shishir Kolathaya </a> 
              <br> (*equal contribution) <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2022 &nbsp <font color="green"></font><br>
							<em>NIPS Deep RL Workshop</em>, 2021 &nbsp <font color="red"><strong>(Poster)</strong></font> <br>
							<em>NIPS Offline RL Workshop</em>, 2021 &nbsp <font color="red"><strong>(Poster)</strong></font><br>
              <br>
              <a href="https://soumyarani.github.io/DMD-MPC-RL/">project page</a>
              /
              <a href="https://arxiv.org/abs/2112.02999">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=Bj9dN1KNPAs">video</a>
							<!--/ -->
              <!-- <a href="https://github.com/soumyarani/mopac">code</a> -->
              <!--<p>Summary: Dynamic Mirror Descent is applied for an H step lookahead policy optimisation to augment the dataset for training an offpolicy RL, improving significantly the sample efficincy of Soft Actor Critic, widely used offpolicy RL algorithm. Further the proposed framework, DeMoRL generalises existing Model Based-Model Free (Mb-Mf) Approaches and acheives state of the art performance in Benchmark MuJoCo Tasks.</p>-->
            </td>
          </tr> 
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()" >
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="http://arxiv.org/abs/2110.12239">
                6. <papertitle>Policy Search using Dynamic Mirror Descent MPC for Model Free Off Policy RL </papertitle>
              </a>
              <br>
              <strong>Soumya R Samineni*</strong>,
              <em>Masters Thesis</em>, 2021 &nbsp <br>
              <br>
              <a href="http://arxiv.org/abs/2110.12239">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=Bj9dN1KNPAs">video</a>
							/
              <a href="https://github.com/soumyarani/mopac">code</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
